<h1 align="center">
	<p align="center">
        🔮 Awesome AI Agents
		<a href="https://x.com/wj_Mcat" target="_blank">
			<img src="https://img.shields.io/twitter/follow/wj_Mcat.svg?logo=twitter">
		</a>
	</p>
</h1>

## 初衷

分享自己在工作学习过程中对于Agent所有的知识点，并定期将其编写成一篇篇博客，进而跟大家讨论学习，共同进步。

## Paper Reading

* **ORPO: Monolithic Preference Optimization without Reference Model**

ORPO 提出了一个非常创新的方法：将 模型对齐阶段 和 SFT阶段 融合到一起，进而提升模型的训练方法。

在 SFT 阶段，就直接将对齐的数据加入到训练当中，进而在SFT 阶段就已经实现了模型对齐的能力。

* **Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models**

`解决的问题`：此论文旨在提升提供一个创建高质量指令跟随数据集的方法，进而提高在不同方法中指令学习的能力。

此论文中是通过生成一个函数函数来检测 Response 内容是否正确，进而提升数据质量。

> 此论文的方法不算是很创新，可是从一定程度上告诉我们：数据质量的重要性。

## Join the community

- Follow us on [X ](https://twitter.com/wj_Mcat)
- [Hit us up on discord](https://discord.gg/gJNKfdTr)
- Get my latest blogs on [知识星球](https://t.zsxq.com/soEav)


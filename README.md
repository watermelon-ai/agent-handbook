# Awesome Agent


## Paper Reading


* **ORPO: Monolithic Preference Optimization without Reference Model**

将DPO 和 SFT 阶段融合到一起，实现在SFT 就已经完成了对齐相关的工作，优点是有效提升训练资源以及降低时间成本。

此方法还有待社区验证是否能完全有效。

